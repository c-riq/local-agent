{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e501686f-323f-4b87-8f9c-8ba89133078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain_community langchain-openai langchain-experimental langchain langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4fb67-113a-4b88-9f93-7e3a95cee035",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "We'll use the Mistral API and `Codestral` instruct model, which support tool use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "982e4609-86e4-4934-828f-e03d89c20393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5fa22a2b-4a6d-4ac4-ae00-36449e926aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3dca9-4485-4ae5-aa87-cd1f02bad8b9",
   "metadata": {},
   "source": [
    "## Code Generation\n",
    "\n",
    "Test with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a188c8ca-c053-4e6d-b7af-38a3b6b371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LLM\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "llm = OllamaFunctions(model=\"codellama\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4191c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5cfb3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "test_gen_prompt = PromptTemplate.from_template((\n",
    "            \"\"\"You are a coding assistant writing test which throw an exception for every failed test for Test Driven Development. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the test, 2) the imports, 3) the functioning test code block.\n",
    "            \\n Human: {question}\n",
    "            AI: \"\"\"))\n",
    "\n",
    "code_gen_prompt = PromptTemplate.from_template((\n",
    "            \"\"\"You are a coding assistant. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block.\n",
    "            \\n Human: {question}. Include the following test code below the implementation: {tests}\n",
    "            AI: \"\"\"))\n",
    "\n",
    "combined_prompt = PromptTemplate.from_template((\n",
    "            \"\"\"You are a coding assistant. Ensure any code or test you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the tests to check corectness of your solution and invocation of these tests, 4) the functioning code block.\n",
    "            \\n Human: {question}\n",
    "            AI: \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c3efe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class test_code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    test: str = Field(description=\"Test code block not including import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(test_code)\n",
    "#test_gen_chain = test_gen_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ccfeaeb3-796e-4710-a526-fe5c605ba268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code_gen_chain = code_gen_prompt | structured_llm\n",
    "combined_chain = combined_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9fc0290d-5a04-4514-8664-91f9dbf2da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a function for fibonacci.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "973281bd-e74b-4386-98c6-210af5e31982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a function for fibonacci.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Write a function for fibonacci.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a coding assistant. Ensure any code or test you provide can be executed with all required imports and variables \\n\\n            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the tests to check corectness of your solution and invocation of these tests, 4) the functioning code block.\\n            \\n Human: Write a function for fibonacci.\\n            AI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:OllamaFunctions] [8.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9fb7dd55-2285-4c96-a4d4-4ea0260e0d92-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"test_code\",\n",
      "                \"args\": {\n",
      "                  \"prefix\": \"Fibonacci sequence\",\n",
      "                  \"imports\": \"from math import floor\\n\",\n",
      "                  \"test\": \"def test_fibonacci(n):\\n    assert floor(fibonacci(n)) == fibonacci(n)\\n\",\n",
      "                  \"code\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\"\n",
      "                },\n",
      "                \"id\": \"call_c33cf49e7bfe47cbab1c6ace7776e28b\"\n",
      "              }\n",
      "            ],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:parse_response] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:parse_response] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{\\\"prefix\\\": \\\"Fibonacci sequence\\\", \\\"imports\\\": \\\"from math import floor\\\\n\\\", \\\"test\\\": \\\"def test_fibonacci(n):\\\\n    assert floor(fibonacci(n)) == fibonacci(n)\\\\n\\\", \\\"code\\\": \\\"def fibonacci(n):\\\\n    if n <= 1:\\\\n        return n\\\\n    else:\\\\n        return fibonacci(n-1) + fibonacci(n-2)\\\"}\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:PydanticOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"{\\\"prefix\\\": \\\"Fibonacci sequence\\\", \\\"imports\\\": \\\"from math import floor\\\\n\\\", \\\"test\\\": \\\"def test_fibonacci(n):\\\\n    assert floor(fibonacci(n)) == fibonacci(n)\\\\n\\\", \\\"code\\\": \\\"def fibonacci(n):\\\\n    if n <= 1:\\\\n        return n\\\\n    else:\\\\n        return fibonacci(n-1) + fibonacci(n-2)\\\"}\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:PydanticOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [8.24s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "from math import floor\n",
      "\n",
      "def test_fibonacci(n):\n",
      "    assert floor(fibonacci(n)) == fibonacci(n)\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "Schema for code solutions to questions about LCEL.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "llm_response = combined_chain.invoke(question)\n",
    "print(llm_response.imports)\n",
    "print(llm_response.test)\n",
    "print(llm_response.code)\n",
    "print(llm_response.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d45a61-945c-42b7-a272-f998c16ab1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "54fe1169-c95a-40ba-a337-68da209f4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(code_gen_chain.invoke({\"question\":question, \"tests\": tests.test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "183d77b8-f180-4815-b39f-8ef507ec0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing import Dict, TypedDict, List\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55043d78-c012-4280-bc8b-259f04a29cb4",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14bc89d1-3ca6-4847-a048-1803e0e4600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "### Parameters\n",
    "max_iterations = 3\n",
    "\n",
    "### Nodes\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = combined_chain.invoke(messages)\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Here is my attempt to solve the problem: {code_solution.prefix} \\n Imports: {code_solution.imports} \\n Test: {code_solution.test} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    prefix = code_solution.prefix\n",
    "    imports = code_solution.imports\n",
    "    test = code_solution.test\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test. Here is the error: {e}. Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, test and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        combined_code = f\"{imports}\\n{code}\\n{test}\"\n",
    "        # Use a shared scope for exec\n",
    "        global_scope = {}\n",
    "        exec(combined_code, global_scope)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test or unit test: {e}) Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, test and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "### Conditional edges\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "### Utilities\n",
    "\n",
    "import uuid \n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2dff2209-44c7-4e2c-b607-ba6675f9e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "builder.add_node(\"generate\", generate)  # generation solution\n",
    "builder.add_node(\"check_code\", code_check)  # check code\n",
    "\n",
    "# Build graph\n",
    "builder.set_entry_point(\"generate\")\n",
    "builder.add_edge(\"generate\", \"check_code\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d4bb21cd-af20-4d4d-89ff-384db034b7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "993ba306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (10.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "242aa2f0-2c31-462f-a958-ff9ae0cf7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Write a Python program that prints 'Hello, World!' to the console.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATING CODE SOLUTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a coding assistant. Ensure any code or test you provide can be executed with all required imports and variables \\n\\n            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the tests to check corectness of your solution and invocation of these tests, 4) the functioning code block.\\n            \\n Human: [HumanMessage(content=\\\"Write a Python program that prints 'Hello, World!' to the console.\\\", id='81365e22-d00a-4115-8b56-aed6665fe09a')]\\n            AI:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:OllamaFunctions] [7.11s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e1b962bc-4232-4c46-b5af-69172a424138-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"test_code\",\n",
      "                \"args\": {\n",
      "                  \"prefix\": \"Print 'Hello, World!' to the console.\",\n",
      "                  \"imports\": \"\",\n",
      "                  \"test\": \"print('Hello, World!')\\n\",\n",
      "                  \"code\": \"print('Hello, World!')\"\n",
      "                },\n",
      "                \"id\": \"call_0db5449806a54eafb7fa7644d0d5246e\"\n",
      "              }\n",
      "            ],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > chain:parse_response] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > chain:parse_response] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"{\\\"prefix\\\": \\\"Print 'Hello, World!' to the console.\\\", \\\"imports\\\": \\\"\\\", \\\"test\\\": \\\"print('Hello, World!')\\\\n\\\", \\\"code\\\": \\\"print('Hello, World!')\\\"}\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:PydanticOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"{\\\"prefix\\\": \\\"Print 'Hello, World!' to the console.\\\", \\\"imports\\\": \\\"\\\", \\\"test\\\": \\\"print('Hello, World!')\\\\n\\\", \\\"code\\\": \\\"print('Hello, World!')\\\"}\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:PydanticOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [7.12s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,error,messages,generation,iterations>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,error,messages,generation,iterations>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [7.12s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is my attempt to solve the problem: Print 'Hello, World!' to the console. \n",
      " Imports:  \n",
      " Test: print('Hello, World!')\n",
      " \n",
      " Code: print('Hello, World!')\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECKING CODE---\n",
      "Hello, World!\n",
      "Hello, World!\n",
      "---NO CODE TEST FAILURES---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code > chain:ChannelWrite<check_code,error,messages,generation,iterations>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code > chain:ChannelWrite<check_code,error,messages,generation,iterations>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code > chain:decide_to_finish] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---DECISION: FINISH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code > chain:decide_to_finish] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"end\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:check_code] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [7.14s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_code(prefix=\"Print 'Hello, World!' to the console.\", imports='', test=\"print('Hello, World!')\\n\", code=\"print('Hello, World!')\", description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)\n",
    "\n",
    "event['generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924e707-5970-4254-a748-fa75628916f2",
   "metadata": {},
   "source": [
    "`Trace:`\n",
    "\n",
    "https://smith.langchain.com/public/53bcdaab-e3c5-4423-9908-c44595325c38/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb883df-540b-46ab-9415-fe27db68456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"I want to vectorize a function\\n\\n        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\\n        for i, val1 in enumerate(rows):\\n            for j, val2 in enumerate(cols):\\n                for j, val3 in enumerate(ch):\\n                    # Assuming you want to store the pair as tuples in the matrix\\n                    frame[i, j, k] = image[val1, val2, val3]\\n\\n        out.write(np.array(frame))\\n\\nwith a simple numpy function that does something like this what is it called. Show me a test case with this working.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"I want to vectorize a function\\n\\n        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\\n        for i, val1 in enumerate(rows):\\n            for j, val2 in enumerate(cols):\\n                for j, val3 in enumerate(ch):\\n                    # Assuming you want to store the pair as tuples in the matrix\\n                    frame[i, j, k] = image[val1, val2, val3]\\n\\n        out.write(np.array(frame))\\n\\nwith a simple numpy function that does something like this what is it called. Show me a test case with this working.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    [\n",
      "      \"user\",\n",
      "      \"I want to vectorize a function\\n\\n        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\\n        for i, val1 in enumerate(rows):\\n            for j, val2 in enumerate(cols):\\n                for j, val3 in enumerate(ch):\\n                    # Assuming you want to store the pair as tuples in the matrix\\n                    frame[i, j, k] = image[val1, val2, val3]\\n\\n        out.write(np.array(frame))\\n\\nwith a simple numpy function that does something like this what is it called. Show me a test case with this working.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"iterations\": 0\n",
      "}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I want to vectorize a function\n",
      "\n",
      "        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
      "        for i, val1 in enumerate(rows):\n",
      "            for j, val2 in enumerate(cols):\n",
      "                for j, val3 in enumerate(ch):\n",
      "                    # Assuming you want to store the pair as tuples in the matrix\n",
      "                    frame[i, j, k] = image[val1, val2, val3]\n",
      "\n",
      "        out.write(np.array(frame))\n",
      "\n",
      "with a simple numpy function that does something like this what is it called. Show me a test case with this working.\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATING CODE SOLUTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a coding assistant. Ensure any code or test you provide can be executed with all required imports and variables \\n\\n            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the tests to check corectness of your solution and invocation of these tests, 4) the functioning code block.\\n            \\n Human: [HumanMessage(content='I want to vectorize a function\\\\n\\\\n        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\\\\n        for i, val1 in enumerate(rows):\\\\n            for j, val2 in enumerate(cols):\\\\n                for j, val3 in enumerate(ch):\\\\n                    # Assuming you want to store the pair as tuples in the matrix\\\\n                    frame[i, j, k] = image[val1, val2, val3]\\\\n\\\\n        out.write(np.array(frame))\\\\n\\\\nwith a simple numpy function that does something like this what is it called. Show me a test case with this working.', id='445a86ce-aa76-4862-a618-fab24b712434')]\\n            AI:\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = '''I want to vectorize a function\n",
    "\n",
    "        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
    "        for i, val1 in enumerate(rows):\n",
    "            for j, val2 in enumerate(cols):\n",
    "                for j, val3 in enumerate(ch):\n",
    "                    # Assuming you want to store the pair as tuples in the matrix\n",
    "                    frame[i, j, k] = image[val1, val2, val3]\n",
    "\n",
    "        out.write(np.array(frame))\n",
    "\n",
    "with a simple numpy function that does something like this what is it called. Show me a test case with this working.'''\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a3292-1e0e-49cf-8b28-bef179afe6a2",
   "metadata": {},
   "source": [
    "Trace w/ good example of self-correction:\n",
    "\n",
    "https://smith.langchain.com/public/b54778a0-d267-4f09-bc28-71761201c522/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05da1f-c272-405d-8a7b-552cfc3106e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = \"\"\"Create a Python program that allows two players to play a game of Tic-Tac-Toe. The game should be played on a 3x3 grid. The program should:\n",
    "\n",
    "- Allow players to take turns to input their moves.\n",
    "- Check for invalid moves (e.g., placing a marker on an already occupied space).\n",
    "- Determine and announce the winner or if the game ends in a draw.\n",
    "\n",
    "Requirements:\n",
    "- Use a 2D list to represent the Tic-Tac-Toe board.\n",
    "- Use functions to modularize the code.\n",
    "- Validate player input.\n",
    "- Check for win conditions and draw conditions after each move.\"\"\"\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d900cd6-2df9-467d-8e74-803527269008",
   "metadata": {},
   "source": [
    "Trace w/ good example of failure to correct:\n",
    "\n",
    "https://smith.langchain.com/public/871ae736-2f77-44d4-b0da-a600d8f5377d/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fc2a4-8e5b-4faa-8f52-3977226bd09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a093a5-7302-4f86-8d4c-625d079e44e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567ea81-f277-4402-a17e-25c628c7273c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
