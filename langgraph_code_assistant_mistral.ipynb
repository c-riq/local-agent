{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e501686f-323f-4b87-8f9c-8ba89133078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain_community langchain-openai langchain-experimental langchain langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4fb67-113a-4b88-9f93-7e3a95cee035",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "We'll use the Mistral API and `Codestral` instruct model, which support tool use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982e4609-86e4-4934-828f-e03d89c20393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3dca9-4485-4ae5-aa87-cd1f02bad8b9",
   "metadata": {},
   "source": [
    "## Code Generation\n",
    "\n",
    "Test with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a188c8ca-c053-4e6d-b7af-38a3b6b371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LLM\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "llm = OllamaFunctions(model=\"codellama\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4191c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfb3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "code_gen_prompt_claude = PromptTemplate.from_template((\n",
    "            \"\"\"You are a coding assistant. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block.\n",
    "            \\n Human: {question}\n",
    "            AI: \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3efe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "# LLM\n",
    "structured_llm = llm.with_structured_output(code)\n",
    "code_gen_chain = code_gen_prompt_claude | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc0290d-5a04-4514-8664-91f9dbf2da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a function for fibonacci.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973281bd-e74b-4386-98c6-210af5e31982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='Function to calculate Fibonacci numbers' imports='' code='def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)' description='Schema for code solutions to questions about LCEL.'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(code_gen_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183d77b8-f180-4815-b39f-8ef507ec0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing import Dict, TypedDict, List\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55043d78-c012-4280-bc8b-259f04a29cb4",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14bc89d1-3ca6-4847-a048-1803e0e4600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "### Parameters\n",
    "max_iterations = 3\n",
    "\n",
    "### Nodes\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(messages)\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Here is my attempt to solve the problem: {code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    prefix = code_solution.prefix\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test. Here is the error: {e}. Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        combined_code = f\"{imports}\\n{code}\"\n",
    "        # Use a shared scope for exec\n",
    "        global_scope = {}\n",
    "        exec(combined_code, global_scope)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}) Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "### Conditional edges\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "### Utilities\n",
    "\n",
    "import uuid \n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dff2209-44c7-4e2c-b607-ba6675f9e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "builder.add_node(\"generate\", generate)  # generation solution\n",
    "builder.add_node(\"check_code\", code_check)  # check code\n",
    "\n",
    "# Build graph\n",
    "builder.set_entry_point(\"generate\")\n",
    "builder.add_edge(\"generate\", \"check_code\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4bb21cd-af20-4d4d-89ff-384db034b7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "993ba306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow\n",
      "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "Successfully installed Pillow-10.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "242aa2f0-2c31-462f-a958-ff9ae0cf7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Write a Python program that prints 'Hello, World!' to the console.\n",
      "---GENERATING CODE SOLUTION---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is my attempt to solve the problem: Print 'Hello, World!' to the console. \n",
      " Imports:  \n",
      " Code: print('Hello, World!')\n",
      "---CHECKING CODE---\n",
      "Hello, World!\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "code(prefix=\"Print 'Hello, World!' to the console.\", imports='', code=\"print('Hello, World!')\", description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)\n",
    "\n",
    "event['generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924e707-5970-4254-a748-fa75628916f2",
   "metadata": {},
   "source": [
    "`Trace:`\n",
    "\n",
    "https://smith.langchain.com/public/53bcdaab-e3c5-4423-9908-c44595325c38/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bb883df-540b-46ab-9415-fe27db68456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I want to vectorize a function\n",
      "\n",
      "        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
      "        for i, val1 in enumerate(rows):\n",
      "            for j, val2 in enumerate(cols):\n",
      "                for j, val3 in enumerate(ch):\n",
      "                    # Assuming you want to store the pair as tuples in the matrix\n",
      "                    frame[i, j, k] = image[val1, val2, val3]\n",
      "\n",
      "        out.write(np.array(frame))\n",
      "\n",
      "with a simple numpy function that does something like this what is it called. Show me a test case with this working.\n",
      "---GENERATING CODE SOLUTION---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is my attempt to solve the problem: Vectorizing a function to write an image to a file using NumPy. \n",
      " Imports: import numpy as np\n",
      "from PIL import Image \n",
      " Code: def vectorize_image(image, out):\n",
      "    # Get the dimensions of the output image\n",
      "    out_h, out_w = out.size\n",
      "\n",
      "    # Initialize a matrix to store the pixel values\n",
      "    frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
      "\n",
      "    # Iterate over each row and column of the image\n",
      "    for i, val1 in enumerate(rows):\n",
      "        for j, val2 in enumerate(cols):\n",
      "            for j, val3 in enumerate(ch):\n",
      "                # Assuming you want to store the pair as tuples in the matrix\n",
      "                frame[i, j, k] = image[val1, val2, val3]\n",
      "\n",
      "    out.write(np.array(frame))\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = '''I want to vectorize a function\n",
    "\n",
    "        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
    "        for i, val1 in enumerate(rows):\n",
    "            for j, val2 in enumerate(cols):\n",
    "                for j, val3 in enumerate(ch):\n",
    "                    # Assuming you want to store the pair as tuples in the matrix\n",
    "                    frame[i, j, k] = image[val1, val2, val3]\n",
    "\n",
    "        out.write(np.array(frame))\n",
    "\n",
    "with a simple numpy function that does something like this what is it called. Show me a test case with this working.'''\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a3292-1e0e-49cf-8b28-bef179afe6a2",
   "metadata": {},
   "source": [
    "Trace w/ good example of self-correction:\n",
    "\n",
    "https://smith.langchain.com/public/b54778a0-d267-4f09-bc28-71761201c522/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05da1f-c272-405d-8a7b-552cfc3106e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Create a Python program that allows two players to play a game of Tic-Tac-Toe. The game should be played on a 3x3 grid. The program should:\n",
      "\n",
      "- Allow players to take turns to input their moves.\n",
      "- Check for invalid moves (e.g., placing a marker on an already occupied space).\n",
      "- Determine and announce the winner or if the game ends in a draw.\n",
      "\n",
      "Requirements:\n",
      "- Use a 2D list to represent the Tic-Tac-Toe board.\n",
      "- Use functions to modularize the code.\n",
      "- Validate player input.\n",
      "- Check for win conditions and draw conditions after each move.\n",
      "---GENERATING CODE SOLUTION---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is my attempt to solve the problem: Tic-Tac-Toe Game \n",
      " Imports: import sys\n",
      "\n",
      " \n",
      " Code: def play_game():\n",
      "\t# Initialize the game board\n",
      "\tboard = [[' ', ' ', ' '], [' ', ' ', ' '], [' ', ' ', ' ']]\n",
      "\n",
      "\t# Set the player's turn to X\n",
      "\tx_player = True\n",
      "\n",
      "\t# Set the winner to None\n",
      "\twinner = None\n",
      "\n",
      "\twhile not winner:\n",
      "\t\t# Get the current player's move\n",
      "\t\tmove = get_move(board, x_player)\n",
      "\n",
      "\t\t# Check if the move is valid\n",
      "\t\tif move[0] < 0 or move[1] < 0 or move[0] > 2 or move[1] > 2:\n",
      "\t\t\tprint('Invalid move. Please try again.')\n",
      "\t\telse:\n",
      "\t\t\t# Make the move on the board\n",
      "\t\t\tboard[move[0]][move[1]] = 'X' if x_player else 'O'\n",
      "\n",
      "\t\t\t# Check for a win condition\n",
      "\t\t\tif check_win(board, 'X') or check_win(board, 'O'):\n",
      "\t\t\t\twinner = get_winner(board)\n",
      "\n",
      "\t\t\t\tprint('Congratulations! Player', winner, 'wins.')\n",
      "\t\t\telse:\n",
      "\t\t\t\tx_player = not x_player\n",
      "\n",
      "\t\t\t\tprint(board)\n",
      "\n",
      "\t\t\t\tif check_draw(board):\n",
      "\t\t\t\t\twinner = 'D'\n",
      "\n",
      "\t\t\t\t\tprint('The game is a draw.')\n",
      "\n",
      "\treturn winner\n",
      "\n",
      "def get_move(board, x_player):\n",
      "\t# Get the player's move\n",
      "\tmove = input('Player ' + ('X' if x_player else 'O') + ', please enter your move (row col): ')\n",
      "\n",
      "\t# Check if the move is valid\n",
      "\tif not move.isdigit() or len(move) != 3:\n",
      "\t\tprint('Invalid move. Please try again.')\n",
      "\telse:\n",
      "\t\treturn [int(move[0]), int(move[1])]\n",
      "\n",
      "def check_win(board, marker):\n",
      "\t# Check if there are three markers in a row\n",
      "\tfor i in range(3):\n",
      "\t\tif board[i][0] == marker and board[i][1] == marker and board[i][2]  ... (truncated)\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = \"\"\"Create a Python program that allows two players to play a game of Tic-Tac-Toe. The game should be played on a 3x3 grid. The program should:\n",
    "\n",
    "- Allow players to take turns to input their moves.\n",
    "- Check for invalid moves (e.g., placing a marker on an already occupied space).\n",
    "- Determine and announce the winner or if the game ends in a draw.\n",
    "\n",
    "Requirements:\n",
    "- Use a 2D list to represent the Tic-Tac-Toe board.\n",
    "- Use functions to modularize the code.\n",
    "- Validate player input.\n",
    "- Check for win conditions and draw conditions after each move.\"\"\"\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d900cd6-2df9-467d-8e74-803527269008",
   "metadata": {},
   "source": [
    "Trace w/ good example of failure to correct:\n",
    "\n",
    "https://smith.langchain.com/public/871ae736-2f77-44d4-b0da-a600d8f5377d/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fc2a4-8e5b-4faa-8f52-3977226bd09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
